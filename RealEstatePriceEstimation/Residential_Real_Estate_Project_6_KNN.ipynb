{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house prices using k-nearest neighbors regression\n",
    "In this notebook, you will implement k-nearest neighbors regression. You will:\n",
    "  * Find the k-nearest neighbors of a given query input\n",
    "  * Predict the output for the query input using the k-nearest neighbors\n",
    "  * Choose the best value of k using a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2450 entries, 0 to 2449\n",
      "Data columns (total 27 columns):\n",
      "SALE TYPE                                                                                     2450 non-null object\n",
      "SOLD DATE                                                                                     965 non-null object\n",
      "PROPERTY TYPE                                                                                 2450 non-null object\n",
      "ADDRESS                                                                                       2438 non-null object\n",
      "CITY                                                                                          2444 non-null object\n",
      "STATE OR PROVINCE                                                                             2450 non-null object\n",
      "ZIP OR POSTAL CODE                                                                            2443 non-null float64\n",
      "PRICE                                                                                         1864 non-null float64\n",
      "BEDS                                                                                          2311 non-null float64\n",
      "BATHS                                                                                         2306 non-null float64\n",
      "LOCATION                                                                                      965 non-null object\n",
      "SQUARE FEET                                                                                   2361 non-null float64\n",
      "LOT SIZE                                                                                      2175 non-null float64\n",
      "YEAR BUILT                                                                                    2373 non-null float64\n",
      "DAYS ON MARKET                                                                                965 non-null float64\n",
      "$/SQUARE FEET                                                                                 1820 non-null float64\n",
      "HOA/MONTH                                                                                     402 non-null float64\n",
      "STATUS                                                                                        1551 non-null object\n",
      "NEXT OPEN HOUSE START TIME                                                                    0 non-null float64\n",
      "NEXT OPEN HOUSE END TIME                                                                      0 non-null float64\n",
      "URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)    2450 non-null object\n",
      "SOURCE                                                                                        1551 non-null object\n",
      "MLS#                                                                                          965 non-null object\n",
      "FAVORITE                                                                                      2450 non-null object\n",
      "INTERESTED                                                                                    2450 non-null object\n",
      "LATITUDE                                                                                      2450 non-null float64\n",
      "LONGITUDE                                                                                     2450 non-null float64\n",
      "dtypes: float64(14), object(13)\n",
      "memory usage: 516.9+ KB\n"
     ]
    }
   ],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Create the same features as you did in Project 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, sqrt\n",
    "sales['sqft_sqrt'] = sales['SQUARE FEET'].apply(sqrt)\n",
    "sales['lot_sqrt'] = sales['LOT SIZE'].apply(sqrt)\n",
    "sales['bedrooms_square'] = sales['BEDS']*sales['BEDS']\n",
    "sales['bath_square'] = sales['BATHS']*sales['BATHS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import useful functions from previous notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To efficiently compute pairwise distances among data points, we will convert the dataframe into a 2D Numpy array. First import the numpy library and then copy and paste `get_numpy_data()` from Project 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data, features, output):\n",
    "    data['Constant'] = 1\n",
    "    features = ['Constant'] + features\n",
    "    feature_matrix = data[features].values\n",
    "    output_array = data[output].values\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the `normalize_features()` function from Project 5 that normalizes all feature columns to unit norm. Paste this function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(feature_matrix):\n",
    "    norms = np.linalg.norm(feature_matrix, axis=0)\n",
    "    features = feature_matrix / norms\n",
    "    \n",
    "    return(features,norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training, test, and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into training/validation/test data with 64/16/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid,test = train_test_split(sales,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training,validation = train_test_split(train_valid,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all of the numerical inputs listed in `all_features`, transform the sales, the training, test, and validation dataframes into Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['SQUARE FEET', 'LOT SIZE', \n",
    "                'BEDS', 'BATHS','YEAR BUILT', 'sqft_sqrt','lot_sqrt','bedrooms_square','bath_square']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales[all_features+['PRICE']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liziwei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "feature_train,output_train = get_numpy_data(training, all_features, \"PRICE\")\n",
    "feature_valid,output_valid = get_numpy_data(validation, all_features, \"PRICE\")\n",
    "feature_testing,output_testing = get_numpy_data(test, all_features, \"PRICE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing distances, it is crucial to normalize features. Otherwise, for example, the `SQUARE FEET` feature (typically on the order of thousands) would exert a much larger influence on distance than the `BEDS` feature (typically on the order of ones). We divide each column of the training feature matrix by its 2-norm, so that the transformed column has unit norm.\n",
    "\n",
    "IMPORTANT: Make sure to store the norms of the features in the sales set. The features in the training, test and validation sets must be divided by these same norms, so that the training, test, and validation sets are normalized consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_norm,norms = normalize_features(feature_train)\n",
    "feature_valid_norm = feature_valid/norms\n",
    "feature_testing_norm = feature_testing/norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code # normalize whole set features (columns)\n",
    "# your code  #normalize training set by the norms\n",
    "# your code # normalize test set by the norms\n",
    "# your code # normalize validation set by the norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute a single distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's just explore computing the \"distance\" between two given houses.  We will take our **query house** to be the first house of the test set and look at the distance between this house and the 10th house of the training set.\n",
    "\n",
    "To see the features associated with the query house, print the 10th row (index 9) of the training feature matrix. You should get an 10-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02525381,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train_norm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02525381,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train_norm[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the 10th row (index 9) of the training feature matrix. Again, you get an 10-dimensional vector with components between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07955573,  0.10427187,  0.06957212,  0.09100315,  0.08508285,\n",
       "        0.07805936,  0.09625196,  0.10177752,  0.08835061,  0.0684989 ])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Euclidean distance between the query house and the 10th house of the training set?\n",
    "Note: Do not use the np.linalg.norm function; use np.sqrt, np.sum, and the power operator (**) instead. The latter approach is more easily adapted to computing multiple distances at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04162270780475861"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.sum((feature_train_norm[1]-feature_train_norm[9])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute multiple distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, to do nearest neighbor regression, we need to compute the distance between our query house and *all* houses in the training set.  \n",
    "\n",
    "To visualize this nearest-neighbor search, let's first compute the distance from our query house (`features_test[0]`) to the first 10 houses of the training set (`features_train[0:10]`) and then search for the nearest neighbor within this small set of houses.  Through restricting ourselves to a small set of houses to begin with, we can visually scan the list of 10 distances to verify that our code for finding the nearest neighbor is working.\n",
    "\n",
    "Write a loop to compute the Euclidean distance from the query house to each of the first 10 houses in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.031840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.171262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.014935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.046056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.019386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.015956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.029291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.004446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.036968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  Distance\n",
       "0      0  0.018817\n",
       "1      1  0.031840\n",
       "2      2  0.171262\n",
       "3      3  0.014935\n",
       "4      4  0.046056\n",
       "5      5  0.019386\n",
       "6      6  0.015956\n",
       "7      7  0.029291\n",
       "8      8  0.004446\n",
       "9      9  0.036968"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = []\n",
    "distt = []\n",
    "dataf1 = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    dist = np.sqrt(np.sum((feature_testing_norm[0]-feature_train_norm[i])**2))\n",
    "    ind.append(i)\n",
    "    distt.append(dist)\n",
    "dataf1['Index'] = ind\n",
    "dataf1['Distance'] = distt\n",
    "dataf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the first 10 training houses, which house is the closest to the query house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index       8.000000\n",
       "Distance    0.004446\n",
       "Name: 8, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataf1.iloc[dataf1['Distance'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is computationally inefficient to loop over computing distances to all houses in our training dataset. Fortunately, many of the Numpy functions can be **vectorized**, applying the same operation over multiple values or vectors.  We now walk through this process.\n",
    "\n",
    "Consider the following loop that computes the element-wise difference between the features of the query house (`features_test[0]`) and the first 3 training houses (`features_train[0:3]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03106849, 0.01890451, 0.00834287, 0.02774212, 0.02470207,\n",
       "       0.0307811 , 0.02535525, 0.02922241, 0.02109783, 0.01517869])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  4.53387728e-03  2.92749186e-03  0.00000000e+00\n",
      "  1.23510335e-02 -4.73312730e-05  3.24859126e-03  5.67881701e-03\n",
      "  0.00000000e+00  1.13840154e-02]\n",
      "[0.         0.01541198 0.00307933 0.00924737 0.01235103 0.00028399\n",
      " 0.00971821 0.00594355 0.01640943 0.01138402]\n",
      "[0.         0.06893736 0.01072246 0.01849475 0.06175517 0.00070997\n",
      " 0.03111993 0.01709895 0.03750726 0.13281351]\n"
     ]
    }
   ],
   "source": [
    "for x in range(3):\n",
    "    z = feature_train_norm[x] - feature_testing_norm[0]\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subtraction operator (`-`) in Numpy is vectorized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  4.53387728e-03,  2.92749186e-03,\n",
       "         0.00000000e+00,  1.23510335e-02, -4.73312730e-05,\n",
       "         3.24859126e-03,  5.67881701e-03,  0.00000000e+00,\n",
       "         1.13840154e-02],\n",
       "       [ 0.00000000e+00,  1.54119786e-02,  3.07933493e-03,\n",
       "         9.24737470e-03,  1.23510335e-02,  2.83987638e-04,\n",
       "         9.71820838e-03,  5.94354667e-03,  1.64094252e-02,\n",
       "         1.13840154e-02],\n",
       "       [ 0.00000000e+00,  6.89373638e-02,  1.07224566e-02,\n",
       "         1.84947494e-02,  6.17551676e-02,  7.09969095e-04,\n",
       "         3.11199250e-02,  1.70989500e-02,  3.75072577e-02,\n",
       "         1.32813513e-01]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train_norm[0:3]-feature_testing_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of this vectorized operation is identical to that of the loop above, which can be verified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# verify that vectorization works\n",
    "results = feature_train_norm[0:3] - feature_testing_norm[0]\n",
    "print (results[0] - (feature_train_norm[0]-feature_testing_norm[0]))\n",
    "# should print all 0's if results[0] == (features_train[0]-features_test[0])\n",
    "print (results[1] - (feature_train_norm[1]-feature_testing_norm[0]))\n",
    "# should print all 0's if results[1] == (features_train[1]-features_test[0])\n",
    "print (results[2] - (feature_train_norm[2]-feature_testing_norm[0]))\n",
    "# should print all 0's if results[2] == (features_train[2]-features_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it is a good idea to write tests like this cell whenever you are vectorizing a complicated operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform 1-nearest neighbor regression\n",
    "\n",
    "Now that we have the element-wise differences, it is not too hard to compute the Euclidean distances between our query house and all of the training houses. First, write a single-line expression to define a variable `diff` such that `diff[i]` gives the element-wise difference between the features of the query house and the `i`-th training house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = feature_train_norm - feature_testing_norm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the code above, run the following cell, which should output a value 0.254292245678:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.254292245678\n"
     ]
    }
   ],
   "source": [
    "# your code # sum of the feature differences between the query and last training house\n",
    "# should print 0.254292245678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in computing the Euclidean distances is to take these feature-by-feature differences in `diff`, square each, and take the sum over feature indices.  That is, compute the sum of square feature differences for each training house (row in `diff`).\n",
    "\n",
    "By default, `np.sum` sums up everything in the matrix and returns a single number. To instead sum only over a row or column, we need to specifiy the `axis` parameter described in the `np.sum` [documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.sum.html). In particular, `axis=1` computes the sum across each row.\n",
    "\n",
    "Below, we compute this sum of square feature differences for all training houses and verify that the output for the 16th house in the training set is equivalent to having examined only the 16th row of `diff` and computing the sum of squares on that row alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001711461522170833\n",
      "0.001711461522170833\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(diff**2, axis=1)[15]) # take sum of squares across each row, and print the 16th sum\n",
    "print (np.sum(diff[15]**2)) # print the sum of squares for the 16th row -- should be same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this result in mind, write a single-line expression to compute the Euclidean distances between the query house and all houses in the training set. Assign the result to a variable `distances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.sqrt(np.sum(diff**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the code above, run the following cell, which should output a value 0.095099460935:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03184004348027965"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095099460935\n"
     ]
    }
   ],
   "source": [
    "# your code # Euclidean distance between the query house and the 101th training house\n",
    "# should print 0.095099460935"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to write a function that computes the distances from a query house to all training houses. The function should take two parameters: (i) the matrix of training features and (ii) the single feature vector associated with the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(feature_matrix,query):\n",
    "    diff2 = feature_matrix - query\n",
    "    dist2 = np.sqrt(np.sum(diff2**2, axis=1))\n",
    "    return dist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Take the query house to be third house of the test set (`features_test[2]`).  What is the index of the house in the training set that is closest to this query house?\n",
    "2.  What is the predicted value of the query house based on 1-nearest neighbor regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = distance(feature_train_norm,feature_testing_norm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003127127090801451"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2600000.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train[x.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform k-nearest neighbor regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k-nearest neighbors, we need to find a set of k houses in the training set closest to a given query house. We then make predictions based on these k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch k-nearest neighbors\n",
    "\n",
    "Using the functions above, implement a function that takes in\n",
    " * the value of k;\n",
    " * the feature matrix for the training houses; and\n",
    " * the feature vector of the query house\n",
    " \n",
    "and returns the indices of the k closest training houses. For instance, with 2-nearest neighbor, a return value of [5, 10] would indicate that the 6th and 11th training houses are closest to the query house.\n",
    "\n",
    "**Hint**: Look at the [documentation for `np.argsort`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(feature_matrix,query,k):\n",
    "    diff3 = feature_matrix - query\n",
    "    dist3 = np.sqrt(np.sum(diff3**2, axis=1))\n",
    "    value = np.argsort(dist3)\n",
    "    return value[0:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the query house to be third house of the test set (`features_test[2]`).  What are the indices of the 4 training houses closest to the query house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([194, 386, 582, 326])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(feature_train_norm,feature_testing[2],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a single prediction by averaging k nearest neighbor outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to find the k-nearest neighbors, write a function that predicts the value of a given query house. **For simplicity, take the average of the prices of the k nearest neighbors in the training set**. The function should have the following parameters:\n",
    " * the value of k;\n",
    " * the feature matrix for the training houses;\n",
    " * the output values (prices) of the training houses; and\n",
    " * the feature vector of the query house, whose price we are predicting.\n",
    " \n",
    "The function should return a predicted value of the query house.\n",
    "\n",
    "**Hint**: You can extract multiple items from a Numpy array using a list of indices. For instance, `output_train[[6, 10]]` returns the prices of the 7th and 11th training houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_estimate(k,feature_matrix,output,query):\n",
    "    val = knn(feature_matrix,query,k)\n",
    "    price1 = output[val]\n",
    "    price2 = price1.mean()\n",
    "    return price2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again taking the query house to be third house of the test set (`features_test[2]`), predict the value of the query house using k-nearest neighbors with `k=4` and the simple averaging method described and implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628750.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_estimate(4,feature_train_norm,output_train,feature_testing_norm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892500.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this predicted value using 4-nearest neighbors to the predicted value using 1-nearest neighbor computed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make multiple predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to predict the value of *each and every* house in a query set. (The query set can be any subset of the dataset, be it the test set or validation set.) The idea is to have a loop where we take each house in the query set as the query house and make a prediction for that specific house. The new function should take the following parameters:\n",
    " * the value of k;\n",
    " * the feature matrix for the training houses;\n",
    " * the output values (prices) of the training houses; and\n",
    " * the feature matrix for the query set.\n",
    " \n",
    "The function should return a set of predicted values, one for each house in the query set.\n",
    "\n",
    "**Hint**: To get the number of houses in the query set, use the `.shape` field of the query features matrix. See [the documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ndarray.shape.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_estimate_multi(k,feature_matrix,output,query):\n",
    "    price = []\n",
    "    for x in range(0,query.shape[0]):\n",
    "        value = price_estimate(k,feature_matrix,output,query[x])\n",
    "        price.append(value)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for the first 10 houses in the test set using k-nearest neighbors with `k=5`. \n",
    "\n",
    "1. What is the index of the house in this query set that has the lowest predicted value? \n",
    "2. What is the predicted value of this house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = price_estimate_multi(5,feature_train_norm,output_train,feature_testing_norm[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1607700.0,\n",
       " 1814800.0,\n",
       " 480000.0,\n",
       " 1014100.0,\n",
       " 1014100.0,\n",
       " 1607700.0,\n",
       " 1014100.0,\n",
       " 1111200.0,\n",
       " 1014100.0,\n",
       " 847000.0]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best value of k using a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There remains a question of choosing the value of k to use in making predictions. Here, we use a validation set to choose this value. Write a loop that does the following:\n",
    "\n",
    "* For `k` in [1, 2, ..., 15]:\n",
    "    * Makes predictions for each house in the VALIDATION set using the k-nearest neighbors from the TRAINING set.\n",
    "    * Computes the RSS for these predictions on the VALIDATION set\n",
    "    * Stores the RSS computed above in `rss_all`\n",
    "* Report which `k` produced the lowest RSS on VALIDATION set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSS</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.112649e+14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.917218e+14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.659491e+14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.529021e+14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.395109e+14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.314940e+14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.342503e+14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.355181e+14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.317323e+14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.185235e+14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.157152e+14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.143622e+14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.111859e+14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.089426e+14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.074834e+14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RSS  Index\n",
       "0   4.112649e+14      1\n",
       "1   2.917218e+14      2\n",
       "2   2.659491e+14      3\n",
       "3   2.529021e+14      4\n",
       "4   2.395109e+14      5\n",
       "5   2.314940e+14      6\n",
       "6   2.342503e+14      7\n",
       "7   2.355181e+14      8\n",
       "8   2.317323e+14      9\n",
       "9   2.185235e+14     10\n",
       "10  2.157152e+14     11\n",
       "11  2.143622e+14     12\n",
       "12  2.111859e+14     13\n",
       "13  2.089426e+14     14\n",
       "14  2.074834e+14     15"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_all = []\n",
    "index = []\n",
    "dataf4 = pd.DataFrame()\n",
    "for k in range(1,16):\n",
    "    c1 = price_estimate_multi(k,feature_train_norm,output_train,feature_valid_norm)\n",
    "    square_error = (c1 - output_valid)**2\n",
    "    sum_square_error = square_error.sum()\n",
    "    rss_all.append(sum_square_error)\n",
    "    index.append(k)\n",
    "dataf4[\"RSS\"] = rss_all\n",
    "dataf4[\"Index\"] = index\n",
    "dataf4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the performance as a function of `k`, plot the RSS on the VALIDATION set for each considered `k` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x113a34950>]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(dataf4[\"Index\"],dataf4[\"RSS\"],'.')\n",
    "plt.plot(dataf4[\"Index\"],dataf4[\"RSS\"],'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the optimal k? What is the RSS on the TEST data using the value of k found above? To be clear, sum over all houses in the TEST set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSS</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.953116e+14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.933997e+14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.759043e+14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.501049e+14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.344911e+14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.314831e+14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.228954e+14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.146828e+14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.076783e+14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.037844e+14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.048708e+14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.059111e+14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.018922e+14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.978470e+14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.001084e+14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RSS  Index\n",
       "0   4.953116e+14      1\n",
       "1   3.933997e+14      2\n",
       "2   3.759043e+14      3\n",
       "3   3.501049e+14      4\n",
       "4   3.344911e+14      5\n",
       "5   3.314831e+14      6\n",
       "6   3.228954e+14      7\n",
       "7   3.146828e+14      8\n",
       "8   3.076783e+14      9\n",
       "9   3.037844e+14     10\n",
       "10  3.048708e+14     11\n",
       "11  3.059111e+14     12\n",
       "12  3.018922e+14     13\n",
       "13  2.978470e+14     14\n",
       "14  3.001084e+14     15"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_all_2 = []\n",
    "index_2 = []\n",
    "dataf5 = pd.DataFrame()\n",
    "for k in range(1,16):\n",
    "    c1 = price_estimate_multi(k,feature_train_norm,output_train,feature_testing_norm)\n",
    "    square_error_2 = (c1 - output_testing)**2\n",
    "    sum_square_error_2 = square_error_2.sum()\n",
    "    rss_all_2.append(sum_square_error_2)\n",
    "    index_2.append(k)\n",
    "dataf5[\"RSS\"] = rss_all_2\n",
    "dataf5[\"Index\"] = index_2\n",
    "dataf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Cosine  Similarity = \\frac{\\sum_{k=1}^n x_i y_i}{\\sqrt{\\sum_{k=1}^n x_i^2}\\sqrt{\\sum_{k=1}^n y_i^2}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Cosine Distance = 1-CosineSimilarity\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may define the cosine funcion according to the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dis(feature_1,feature_2):\n",
    "    dist22 = []\n",
    "    for x in range(feature_1.shape[0]):\n",
    "        for y in range(feature_2.shape[0]):\n",
    "            sim = (np.dot(feature_1[x],feature_2[y])).sum()/sqrt((np.dot(feature_1[x],feature_1[x])).sum())*sqrt((np.dot(feature_2[y],feature_2[y])).sum())\n",
    "            dis = 1 - sim\n",
    "        dist22.append(dis)\n",
    "    return dist22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_dis(feature_train_norm,feature_testing_norm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, we can get distance from one point from test set to every point in train \n",
    "# size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting each point, we need to sort the values from the smallest to the largest and pick out the first k values according to the KNN algorithm.\n",
    "\n",
    "Then, I will write another function include K and print out the first K values's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_d_k(k,feature_1,feature_2):\n",
    "    dista = cos_dis(feature_1,feature_2)\n",
    "    values = np.argsort(dista)\n",
    "    return values[0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([843, 937, 919, 272])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_d_k(4,feature_train_norm,feature_testing_norm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define a new function including output and return the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_d_k_pri(k,feature_1,feature_2,output):\n",
    "    for x in range(0,k):\n",
    "        ind = cos_d_k(k,feature_1,feature_2)\n",
    "        pre = output[ind]\n",
    "        pre2 = pre.mean()\n",
    "        return pre2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009125.0"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_d_k_pri(4,feature_train_norm,feature_testing_norm[0],output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_d_mean(k,feature_1,feature_2,output):\n",
    "    price3 = []\n",
    "    for x in range(1,k+1):\n",
    "        va = cos_d_k_pri(x,feature_1,feature_2,output)\n",
    "        price3.append(va)\n",
    "    return price3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1850000.0, 1750000.0, 1280000.0, 1009125.0, 932300.0]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_d_mean(5,feature_train_norm,feature_testing_norm[0],output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can write a function or loop to calculate the RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101294755625.0"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = output_testing[0]\n",
    "pred = cos_d_mean(5,feature_train_norm,feature_testing_norm[0],output_train)\n",
    "ss = (original-pred)*(original-pred)\n",
    "rss = ss.sum()\n",
    "rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, we write a function for k between 1 and 10 to calculate the rss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_2 = []\n",
    "for k in range(1,15+1):\n",
    "    original2 = output_testing[0]\n",
    "    pred2 = cos_d_k_pri(k,feature_train_norm,feature_testing_norm[0],output_train)\n",
    "    ss2 = (original2-pred2)*(original2-pred2)\n",
    "    rss2 = ss2.sum()\n",
    "    rss_2.append(rss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "original2 = output_testing[0]\n",
    "pred2 = cos_d_k_pri(4,feature_train_norm,feature_testing_norm[0],output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2896804000000.0,\n",
       " 2566404000000.0,\n",
       " 1281424000000.0,\n",
       " 741536265625.0,\n",
       " 615126490000.0,\n",
       " 438244000000.0,\n",
       " 426782224489.79596,\n",
       " 690976562500.0,\n",
       " 1272384000000.0,\n",
       " 1064817610000.0,\n",
       " 869301950413.2234,\n",
       " 1421658777777.7776,\n",
       " 1225278698224.8523,\n",
       " 1576100897959.1836,\n",
       " 1425317617777.778]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then we write a loop to calculate all the rss in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a test for k from 1 to 5 and calculate rss using data from testing points top 6 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_3 = []\n",
    "for k in range(1,5+1):\n",
    "    rss4 = []\n",
    "    for y in range(output_testing[0:6].shape[0]):\n",
    "        original3 = output_testing[y]\n",
    "        pred3 = cos_d_k_pri(k,feature_train_norm,feature_testing_norm[y],output_train)\n",
    "        ss3 = (original3-pred3)*(original3-pred3)\n",
    "        rss3 = ss3.sum()\n",
    "        rss4.append(rss3)\n",
    "    rss5 = sum(rss4)/len(rss4)\n",
    "    rss_3.append(rss5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3742547333333.3335,\n",
       " 3710614000000.0,\n",
       " 3828427333333.3335,\n",
       " 4097011182291.6665,\n",
       " 4199898423333.3335]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then, we can get the ssr for k value from 1 to 5. We can notice that ssr is smallest\n",
    "# when k = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "JaccardIndex = \\frac{|A ∩ B|}{|A ∪ B|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "JaccardDistance = 1 - JaccardIndex\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "j1 = feature_train_norm[0]\n",
    "j2 = feature_testing_norm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will write a function to find the Jaccard Distance between two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_d(lst1, lst2): \n",
    "    intersect =  len(list(set(lst1) & set(lst2)))\n",
    "    union = len(set(lst1) | set(lst2))\n",
    "    index = intersect / union\n",
    "    distance = 1 - index\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run this function to see if we can get a distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jaccard_d(j1,j2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can write a function to sort the values from the smallest to the largest, and print out the first k values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac_pri(lst1, lst2,k,output):\n",
    "    dista = []\n",
    "    for x in range(len(lst1)):\n",
    "        distan = Jaccard_d(lst1[x],lst2)\n",
    "        dista.append(distan)\n",
    "    dista2 = np.argsort(dista)\n",
    "    value = output[dista2]\n",
    "    return value[0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1034000.,  209000., 1080000.,  108000.,  203000.])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_pri(feature_train_norm, feature_testing_norm[0],5,output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we still take the average of the prices of the k nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac_pri_mean(lst1, lst2,k,output):\n",
    "    final = jac_pri(lst1, lst2,k,output).mean()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034000.0"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_pri_mean(feature_train_norm, feature_testing_norm[0],1,output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we write a loop to calculate ssr for k from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = []\n",
    "for k in range(1,15+1):\n",
    "    v1 = jac_pri_mean(feature_train_norm, feature_testing_norm[0],k,output_train)\n",
    "    v2 = output_testing[0]\n",
    "    error_square = (v1-v2)**2\n",
    "    sr.append(error_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784996000000.0,\n",
       " 224202250000.0,\n",
       " 392293444444.4445,\n",
       " 211370062500.0,\n",
       " 143489440000.0,\n",
       " 267978777777.77774,\n",
       " 294383755102.0408,\n",
       " 240100000000.0,\n",
       " 214883753086.4197,\n",
       " 221276160000.0,\n",
       " 241081000000.0,\n",
       " 215528062500.0,\n",
       " 190297284023.66864,\n",
       " 165649000000.0,\n",
       " 193600000000.0]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we will define a function to calculate all the data in testing and its SSR for k from 1 to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssr = []\n",
    "for k in range(1,15+1):\n",
    "    for i in range(feature_testing_norm.shape[0]):\n",
    "        sr2 = []\n",
    "        v11 = jac_pri_mean(feature_train_norm, feature_testing_norm[i],k,output_train)\n",
    "        v12 = feature_testing_norm[i]\n",
    "        square_error = (v11-v12)**2\n",
    "        sr2.append(square_error)\n",
    "    sr3 = sum(sr2)\n",
    "    ssr.append(sr3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(0,len(ssr)):\n",
    "    sums = ssr[i].sum()\n",
    "    x.append(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = np.arange(1,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataja = pd.DataFrame(x,index=sss,columns=['SSR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSR    3.422016e+12\n",
       "Name: 5, dtype: float64"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataja.iloc[dataja['SSR'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, we can see that when k = 5, the ssr is the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manhattan Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Manhattan Distance = {\\sum_{i=1}^n |x_i - x'_i|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can calculate the Manhattan Distance using one poine from training and one point from test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = feature_testing_norm[2]\n",
    "m2 = feature_train_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14977521429954205"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs((m2-m1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we can check the distance between one point in testing and each point in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for x in range(feature_train_norm.shape[0]):\n",
    "    l1 = abs((feature_train_norm[x]-m1)).sum()\n",
    "    length.append(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14977521429954205,\n",
       " 0.10602280429975747,\n",
       " 0.1895916303892974,\n",
       " 0.21013688857696322,\n",
       " 0.08508460230110693,\n",
       " 0.14750395835586166,\n",
       " 0.20329692988517228,\n",
       " 0.11693362722342074,\n",
       " 0.18165547669893314,\n",
       " 0.17210963608395147,\n",
       " 0.06919076604557536,\n",
       " 0.20144614264492083,\n",
       " 0.29485595119022445,\n",
       " 0.22013775027847124,\n",
       " 0.2008692366076687,\n",
       " 0.10965381371549876,\n",
       " 0.15803977621514917,\n",
       " 0.157221072422005,\n",
       " 0.0880457237297626,\n",
       " 0.01482486079420342,\n",
       " 0.14079681392586774,\n",
       " 0.20800730753386215,\n",
       " 0.05789970597379641,\n",
       " 0.07505812083807654,\n",
       " 0.2068498316034374,\n",
       " 0.21411815269449974,\n",
       " 0.19659719076021093,\n",
       " 0.20382522167037806,\n",
       " 0.07533006956348834,\n",
       " 0.06736359175190429,\n",
       " 0.14969389686265877,\n",
       " 0.15405393245367097,\n",
       " 0.14298568670688694,\n",
       " 0.052566166320203216,\n",
       " 0.14283448938431897,\n",
       " 0.1836222638051755,\n",
       " 0.18206931794559245,\n",
       " 0.06609261022093779,\n",
       " 0.10560632513776314,\n",
       " 0.2455504378124801,\n",
       " 0.13045813671657924,\n",
       " 0.07094357587565908,\n",
       " 0.05919982108235126,\n",
       " 0.14572601091111662,\n",
       " 0.1375090795727889,\n",
       " 0.20364915840802517,\n",
       " 0.008444199973565096,\n",
       " 0.03983686168061075,\n",
       " 0.15530808762894244,\n",
       " 0.1995907612071183,\n",
       " 0.17960387298320163,\n",
       " 0.16075625904339952,\n",
       " 0.12481826380970323,\n",
       " 0.127527163975146,\n",
       " 0.10472761757731784,\n",
       " 0.009871404352002903,\n",
       " 0.11927395398278981,\n",
       " 0.15022697207467461,\n",
       " 0.1252863055765841,\n",
       " 0.08453449668101573,\n",
       " 0.12240217035711029,\n",
       " 0.15585694403359807,\n",
       " 0.1437887475506887,\n",
       " 0.05997070927917479,\n",
       " 0.1791998593113539,\n",
       " 0.054725406159483424,\n",
       " 0.14577527333556334,\n",
       " 0.08506242042295288,\n",
       " 0.07517655811405363,\n",
       " 0.14260972294624732,\n",
       " 0.1613209265746272,\n",
       " 0.40167214649383853,\n",
       " 0.08574955257005804,\n",
       " 0.15407940165986866,\n",
       " 0.0064578734785239525,\n",
       " 0.15551023347554851,\n",
       " 0.15306175385092746,\n",
       " 0.15820301104469173,\n",
       " 0.10200727607049645,\n",
       " 0.15955732549437868,\n",
       " 0.2183605959581023,\n",
       " 0.15543605567551833,\n",
       " 0.2043074476220019,\n",
       " 0.14341359753614666,\n",
       " 0.2226115804221995,\n",
       " 0.09930748803460429,\n",
       " 0.25385648670418703,\n",
       " 0.1531303165201252,\n",
       " 0.1868239459740314,\n",
       " 0.1361836908817794,\n",
       " 0.11323387426986556,\n",
       " 0.09150026461818592,\n",
       " 0.09232805357810589,\n",
       " 0.1555472056518029,\n",
       " 0.10404552713047838,\n",
       " 0.1984756402820435,\n",
       " 0.15369175138586072,\n",
       " 0.24143292647131986,\n",
       " 0.12046598517777726,\n",
       " 0.07648101990278391,\n",
       " 0.1924720478650108,\n",
       " 0.25013614385132826,\n",
       " 0.037575765214424676,\n",
       " 0.07509402892753503,\n",
       " 0.12503540986943598,\n",
       " 0.1455372643690795,\n",
       " 0.0786372308354017,\n",
       " 0.15887304780179545,\n",
       " 0.018777332614291393,\n",
       " 0.1556565108057412,\n",
       " 0.10894537225473083,\n",
       " 0.1568015082405751,\n",
       " 0.20174977855867654,\n",
       " 0.15052314294969898,\n",
       " 0.1693945265363126,\n",
       " 0.07968719685811804,\n",
       " 0.10325458894982312,\n",
       " 0.19205182327462056,\n",
       " 0.1538381782094144,\n",
       " 0.11135120221781386,\n",
       " 0.14134396700300175,\n",
       " 0.12210557308922709,\n",
       " 0.06932013078994059,\n",
       " 0.1396972652359553,\n",
       " 0.19675916361427193,\n",
       " 0.08737846215694137,\n",
       " 0.011789484576732196,\n",
       " 0.21421997675886512,\n",
       " 0.07033356085805026,\n",
       " 0.24259505931661934,\n",
       " 0.19646694480666652,\n",
       " 0.19849911981729945,\n",
       " 0.09175837124434416,\n",
       " 0.06194700043434441,\n",
       " 0.11873702901186498,\n",
       " 0.13638937248713706,\n",
       " 0.15374164488302441,\n",
       " 0.15122404157218344,\n",
       " 0.15201691608469023,\n",
       " 0.14447199592651733,\n",
       " 0.16896146879718704,\n",
       " 0.20422121493083445,\n",
       " 0.0750129879719233,\n",
       " 0.07852649119331037,\n",
       " 0.18703891481871887,\n",
       " 0.14083729019065716,\n",
       " 0.12439029886370956,\n",
       " 0.13386756655395599,\n",
       " 0.12105572490295352,\n",
       " 0.1508456939862437,\n",
       " 0.1477837740942337,\n",
       " 0.13448935917612784,\n",
       " 0.1956527649298013,\n",
       " 0.14750278651928386,\n",
       " 0.05893113304760884,\n",
       " 0.1467256116880916,\n",
       " 0.24928806994341815,\n",
       " 0.1115668545120091,\n",
       " 0.01968785587228916,\n",
       " 0.1721448552870886,\n",
       " 0.12126837665148929,\n",
       " 0.13602731671716695,\n",
       " 0.08515066935013388,\n",
       " 0.2246052343786186,\n",
       " 0.1363134055808692,\n",
       " 0.21128789509697193,\n",
       " 0.22473876516163124,\n",
       " 0.18260448804936924,\n",
       " 0.10838906165348886,\n",
       " 0.14457553762714742,\n",
       " 0.08416501698463802,\n",
       " 0.07445063837579396,\n",
       " 0.1971783712749682,\n",
       " 0.1510963586151193,\n",
       " 0.09686165807824451,\n",
       " 0.08574173426206089,\n",
       " 0.15998844360701184,\n",
       " 0.18955014023321204,\n",
       " 0.19779744921329906,\n",
       " 0.2226115804221995,\n",
       " 0.0943851001353756,\n",
       " 0.08300877999434678,\n",
       " 0.14315081746699082,\n",
       " 0.13905479766212378,\n",
       " 0.08563950489906708,\n",
       " 0.06414157536827936,\n",
       " 0.20225120446096412,\n",
       " 0.13283136528150175,\n",
       " 0.08804441238933565,\n",
       " 0.13105736622124614,\n",
       " 0.052312696475671536,\n",
       " 0.15434505131366802,\n",
       " 0.15861505108679477,\n",
       " 0.15043626406237381,\n",
       " 0.8075552502542938,\n",
       " 0.1419986882067394,\n",
       " 0.0598458955132016,\n",
       " 0.18329338834989356,\n",
       " 0.1593811349687894,\n",
       " 0.15739530183514508,\n",
       " 0.14417928000408226,\n",
       " 0.15821635919901936,\n",
       " 0.10927039792518403,\n",
       " 0.05132424561677454,\n",
       " 0.11617638325403695,\n",
       " 0.13036966023005422,\n",
       " 0.058434750245437116,\n",
       " 0.13332017922792616,\n",
       " 0.04633641063709308,\n",
       " 0.10474453760054575,\n",
       " 0.11194954845828811,\n",
       " 0.15425949858317084,\n",
       " 0.15816035453310223,\n",
       " 0.13382504729283135,\n",
       " 0.050434952544780434,\n",
       " 0.07853584127914548,\n",
       " 0.09889025417799471,\n",
       " 0.20669995303649052,\n",
       " 0.20163451143976818,\n",
       " 0.1710829206971037,\n",
       " 0.14353092923421729,\n",
       " 0.22441529544511518,\n",
       " 0.06567436506805488,\n",
       " 0.07398956504451784,\n",
       " 0.09178536154745134,\n",
       " 0.14958555329790968,\n",
       " 0.06571493082917815,\n",
       " 0.10692845985459391,\n",
       " 0.18738377696507147,\n",
       " 0.16819144951017215,\n",
       " 0.13678115269786564,\n",
       " 0.15052182427317343,\n",
       " 0.22365183597312546,\n",
       " 0.11431566724065514,\n",
       " 0.1139860795779018,\n",
       " 0.5996636586089881,\n",
       " 0.2101067036356565,\n",
       " 0.03239004418154985,\n",
       " 0.10271679797800923,\n",
       " 0.09955669891103641,\n",
       " 0.19938326776101356,\n",
       " 0.09238759643615577,\n",
       " 0.08483282779702936,\n",
       " 0.15738223024415876,\n",
       " 0.08651104415091876,\n",
       " 0.05749975964737006,\n",
       " 0.15552098573871917,\n",
       " 0.11149672696652019,\n",
       " 0.12882704526473937,\n",
       " 0.16956132736548277,\n",
       " 0.1439301514681639,\n",
       " 0.0906745317430965,\n",
       " 0.19229118510191306,\n",
       " 0.08539782187087536,\n",
       " 0.1818216940592944,\n",
       " 0.12547224197467258,\n",
       " 0.05615280942229564,\n",
       " 0.09199032400535681,\n",
       " 0.12219083352815387,\n",
       " 0.14728198385486918,\n",
       " 0.20617977318455538,\n",
       " 0.09073072156977516,\n",
       " 0.09107015548453229,\n",
       " 0.11724129522795687,\n",
       " 0.0699991489195565,\n",
       " 0.10727763937066429,\n",
       " 0.13878549893738018,\n",
       " 0.20341853340529795,\n",
       " 0.13919301346270987,\n",
       " 0.07624265849187992,\n",
       " 0.0765172821109428,\n",
       " 0.08414938604949784,\n",
       " 0.06410546231835362,\n",
       " 0.16186963912292832,\n",
       " 0.24883580286571727,\n",
       " 0.1406538357689867,\n",
       " 0.14773898720352177,\n",
       " 0.07558225106806771,\n",
       " 0.22159717804573034,\n",
       " 0.19971996616590318,\n",
       " 0.16854086770460389,\n",
       " 0.15622371456311984,\n",
       " 0.19875053535670348,\n",
       " 0.05186023331967024,\n",
       " 0.040561400745324626,\n",
       " 0.13067387377008877,\n",
       " 0.14736909531080628,\n",
       " 0.20289103046847157,\n",
       " 0.09469754983626666,\n",
       " 0.14066803699605943,\n",
       " 0.07781226303298408,\n",
       " 0.2214652791978761,\n",
       " 0.15480768031638753,\n",
       " 0.147597457837927,\n",
       " 0.14555290072370003,\n",
       " 0.14134696463382895,\n",
       " 0.15396328370690499,\n",
       " 0.19890208111223984,\n",
       " 0.20032232520557053,\n",
       " 0.15473599912940877,\n",
       " 0.1393345606592096,\n",
       " 0.1607468977809014,\n",
       " 0.15607416869487478,\n",
       " 0.1396730283666608,\n",
       " 0.1177423226194794,\n",
       " 0.1974126314827574,\n",
       " 0.25655648738538206,\n",
       " 0.13987335580536975,\n",
       " 0.14403267468894654,\n",
       " 0.24244245711130175,\n",
       " 0.17953173647103005,\n",
       " 0.12664267008890567,\n",
       " 0.16130956996675924,\n",
       " 0.14625332604715116,\n",
       " 0.1520839578947154,\n",
       " 0.24624853767469923,\n",
       " 0.0775498829084171,\n",
       " 0.204369180810609,\n",
       " 0.10610182406225852,\n",
       " 0.22844030364616924,\n",
       " 0.1882794796495643,\n",
       " 0.115933378294992,\n",
       " 0.1818216940592944,\n",
       " 0.25013614385132826,\n",
       " 0.15261805048351834,\n",
       " 0.08690600829248792,\n",
       " 0.47741013063163534,\n",
       " 0.08148022550151698,\n",
       " 0.12813899951331015,\n",
       " 0.17072008062339028,\n",
       " 0.15826001468751924,\n",
       " 0.09358848322903032,\n",
       " 0.1990440249337101,\n",
       " 0.0327243022807019,\n",
       " 0.14024901443561238,\n",
       " 0.10034830170576066,\n",
       " 0.23329519268665572,\n",
       " 0.15870934397153372,\n",
       " 0.11892388805400515,\n",
       " 0.08580836316966067,\n",
       " 0.15086075199330828,\n",
       " 0.23088369797309485,\n",
       " 0.14261559130591034,\n",
       " 0.031703820306663806,\n",
       " 0.07537571625395215,\n",
       " 0.15371646502815156,\n",
       " 0.24093699572489974,\n",
       " 0.21037885828604513,\n",
       " 0.14792965771759695,\n",
       " 0.0703704325928814,\n",
       " 0.14968002439832728,\n",
       " 0.26588540233736707,\n",
       " 0.08543435708688993,\n",
       " 0.15273292592372256,\n",
       " 0.1449439411275456,\n",
       " 0.05522034411444748,\n",
       " 0.14393426575269422,\n",
       " 0.16325706279534977,\n",
       " 0.15375115263145062,\n",
       " 0.24568074666591655,\n",
       " 0.15261943253409674,\n",
       " 0.0763747635343335,\n",
       " 0.05363010805878698,\n",
       " 0.05314879757733016,\n",
       " 0.14319057110256472,\n",
       " 0.13380951046044146,\n",
       " 0.14863416211109406,\n",
       " 0.1174436347544161,\n",
       " 0.09493282979377796,\n",
       " 0.19746318828581658,\n",
       " 0.21765414267916738,\n",
       " 0.13519776309281722,\n",
       " 0.09756302880343456,\n",
       " 0.058771804070838464,\n",
       " 0.09349823788353154,\n",
       " 0.06000885219952031,\n",
       " 0.06485841809403828,\n",
       " 0.14322700045136963,\n",
       " 0.07886425147271561,\n",
       " 0.027300116412388636,\n",
       " 0.08000063289258769,\n",
       " 0.14251513128513116,\n",
       " 0.036783642029229846,\n",
       " 0.10644461707240066,\n",
       " 0.0989424863474527,\n",
       " 0.18218786378857568,\n",
       " 0.8066572957961206,\n",
       " 0.13813895241157936,\n",
       " 0.03185223362863841,\n",
       " 0.17544603528262387,\n",
       " 0.1607970290385193,\n",
       " 0.20023953470270472,\n",
       " 0.20162187015104216,\n",
       " 0.08503610702621327,\n",
       " 0.029331373595562805,\n",
       " 0.1914590073342572,\n",
       " 0.15420157052012556,\n",
       " 0.10912779071059331,\n",
       " 0.17148550372818205,\n",
       " 0.09348558902816248,\n",
       " 0.045168720187923006,\n",
       " 0.14879797517548046,\n",
       " 0.1367563366562593,\n",
       " 0.14634502867527358,\n",
       " 0.12582244404553838,\n",
       " 0.07923478776392234,\n",
       " 0.17377609948055617,\n",
       " 0.12414002677193256,\n",
       " 0.16007320332509378,\n",
       " 0.2443234889551861,\n",
       " 0.12396418109756006,\n",
       " 0.15024004020545884,\n",
       " 0.08279665029367922,\n",
       " 0.10488489776948957,\n",
       " 0.07044263325230873,\n",
       " 0.15436473559639405,\n",
       " 0.13585571420570577,\n",
       " 0.15053251183144895,\n",
       " 0.10364799475011473,\n",
       " 0.07442543210114219,\n",
       " 0.07106720848054922,\n",
       " 0.13633986592333794,\n",
       " 0.13827884048525635,\n",
       " 0.1654368479108545,\n",
       " 0.15822533307705694,\n",
       " 0.19649451436469317,\n",
       " 0.06711722559724942,\n",
       " 0.16050346263850224,\n",
       " 0.07892250491377079,\n",
       " 0.14219036381672032,\n",
       " 0.1205694500625307,\n",
       " 0.08781624607884941,\n",
       " 0.10216738258575297,\n",
       " 0.09468285276458696,\n",
       " 0.11736626348234552,\n",
       " 0.19413670635779434,\n",
       " 0.12429039170261624,\n",
       " 0.15738261219095703,\n",
       " 0.15784665495777936,\n",
       " 0.06861201911590893,\n",
       " 0.09171103997131766,\n",
       " 0.05557740720987864,\n",
       " 0.1114186411209219,\n",
       " 0.1503904102332802,\n",
       " 0.10807864787727972,\n",
       " 0.15876376390333036,\n",
       " 0.08518406871381759,\n",
       " 0.10071676885431566,\n",
       " 0.1724996195707522,\n",
       " 0.06335503050921412,\n",
       " 0.15282791016583208,\n",
       " 0.0676183580805558,\n",
       " 0.15309057465967926,\n",
       " 0.1314932613445525,\n",
       " 0.13924267708590587,\n",
       " 0.1444906765970158,\n",
       " 0.05120033687797004,\n",
       " 0.13192610493686968,\n",
       " 0.16825770933797715,\n",
       " 0.16635155901412702,\n",
       " 0.2491722184735019,\n",
       " 0.05460722186971607,\n",
       " 0.1506797174154152,\n",
       " 0.14287195283034848,\n",
       " 0.18130163698871726,\n",
       " 0.14412822626147595,\n",
       " 0.009363780904409424,\n",
       " 0.18343833623480052,\n",
       " 0.07534007810292995,\n",
       " 0.14408833750855826,\n",
       " 0.11064127005777076,\n",
       " 0.13563246403292353,\n",
       " 0.08510512551053279,\n",
       " 0.19849911981729945,\n",
       " 0.12802083834795344,\n",
       " 0.10838386434233382,\n",
       " 0.051892092157123204,\n",
       " 0.17635423732224115,\n",
       " 0.0882772879412005,\n",
       " 0.12351855478150583,\n",
       " 0.2440601040482762,\n",
       " 0.12487484855576611,\n",
       " 0.14263485002963258,\n",
       " 0.09514880680016122,\n",
       " 0.4666475417215668,\n",
       " 0.13146971324616277,\n",
       " 0.12145397775205208,\n",
       " 0.10557658918279098,\n",
       " 0.11311688449904239,\n",
       " 0.04313070346488168,\n",
       " 0.13833253068115794,\n",
       " 0.24168421275597698,\n",
       " 0.15529048406172183,\n",
       " 0.15567337686764904,\n",
       " 0.16077509229780843,\n",
       " 0.15768375613186117,\n",
       " 0.12729771298584103,\n",
       " 0.18597424866440432,\n",
       " 0.1557606692648424,\n",
       " 0.1601471946544028,\n",
       " 0.08493563394959416,\n",
       " 0.14935702086968264,\n",
       " 0.15351195903135972,\n",
       " 0.09848645093155878,\n",
       " 0.13989247375212924,\n",
       " 0.1271487521324636,\n",
       " 0.058969679003565305,\n",
       " 0.06796048861885583,\n",
       " 0.15016144968747114,\n",
       " 0.14172570366470819,\n",
       " 0.15751279050209607,\n",
       " 0.12658663761528538,\n",
       " 0.09423574284415201,\n",
       " 0.16174068761552912,\n",
       " 0.1727665084821891,\n",
       " 0.08394802031605209,\n",
       " 0.11454906097746186,\n",
       " 0.18097595749954087,\n",
       " 0.14529503166372312,\n",
       " 0.15275265160991697,\n",
       " 0.0962521634098859,\n",
       " 0.025835796267703808,\n",
       " 0.18418622484517047,\n",
       " 0.14635140419794881,\n",
       " 0.05136763840692066,\n",
       " 0.12953750621338933,\n",
       " 0.18622750252628378,\n",
       " 0.05291407094719483,\n",
       " 0.13301061886610366,\n",
       " 0.06852243251434993,\n",
       " 0.23064542027388216,\n",
       " 0.17008514402485564,\n",
       " 0.044678074869978504,\n",
       " 0.11239753025419533,\n",
       " 0.054937802050883014,\n",
       " 0.08766379457747454,\n",
       " 0.10625707642027363,\n",
       " 0.05474096322474311,\n",
       " 0.20419467653276152,\n",
       " 0.21167935534518884,\n",
       " 0.15263179884181224,\n",
       " 0.18603654548443055,\n",
       " 0.14215384859920496,\n",
       " 0.13565598164279596,\n",
       " 0.12966699629773182,\n",
       " 0.02711010308946052,\n",
       " 0.10419724672068598,\n",
       " 0.06867505018529983,\n",
       " 0.12445284967164705,\n",
       " 0.1273808827185791,\n",
       " 0.11622459741608718,\n",
       " 0.050359301050890995,\n",
       " 0.08277598680243678,\n",
       " 0.19123310890946932,\n",
       " 0.15618791930935813,\n",
       " 0.13530493625620713,\n",
       " 0.1581195561643305,\n",
       " 0.0771060078176633,\n",
       " 0.1473316070121474,\n",
       " 0.1327361755819166,\n",
       " 0.17562062323936872,\n",
       " 0.07731503979773606,\n",
       " 0.10410223179914668,\n",
       " 0.15680885747994028,\n",
       " 0.04179498658578804,\n",
       " 0.18850110520459187,\n",
       " 0.1520884041511392,\n",
       " 0.11431329915727154,\n",
       " 0.05711559113890773,\n",
       " 0.16463572456422915,\n",
       " 0.1698805609690816,\n",
       " 0.144559448346963,\n",
       " 0.2058384602100993,\n",
       " 0.23817697435231194,\n",
       " 0.19774963508026164,\n",
       " 0.1840102895490342,\n",
       " 0.06249896957674006,\n",
       " 0.18027432140821703,\n",
       " 0.08278575778103539,\n",
       " 0.10794713053170588,\n",
       " 0.17868041109580146,\n",
       " 0.08141993924762436,\n",
       " 0.8127864721154867,\n",
       " 0.20085602154724863,\n",
       " 0.094347410929045,\n",
       " 0.16802179407854329,\n",
       " 0.16380199702493414,\n",
       " 0.15975177109091282,\n",
       " 0.1890655883128843,\n",
       " 0.1114661714896236,\n",
       " 0.20267092068352144,\n",
       " 0.11876340959741036,\n",
       " 0.1277146018256221,\n",
       " 0.20517718868030188,\n",
       " 0.11074393101145924,\n",
       " 0.17642149439962115,\n",
       " 0.1222086692579394,\n",
       " 0.06455981471247288,\n",
       " 0.016007108163824585,\n",
       " 0.15765312544804855,\n",
       " 0.0997381904034671,\n",
       " 0.08740999755220918,\n",
       " 0.14446691053010485,\n",
       " 0.1431290964964344,\n",
       " 0.12139438982904568,\n",
       " 0.20855945971634637,\n",
       " 0.11181305461194646,\n",
       " 0.14110245111914574,\n",
       " 0.13286029783925818,\n",
       " 0.1190895379878244,\n",
       " 0.15784337054238975,\n",
       " 0.19933473818052574,\n",
       " 0.052909267790423126,\n",
       " 0.2107143454601447,\n",
       " 0.22959810090349933,\n",
       " 0.05126428985351379,\n",
       " 0.0886746456269191,\n",
       " 0.10468664276842457,\n",
       " 0.0958941693687034,\n",
       " 0.12373860699693037,\n",
       " 0.15449840667079245,\n",
       " 0.22584053155133146,\n",
       " 0.05798718469746287,\n",
       " 0.18386062568447142,\n",
       " 0.1307377520908827,\n",
       " 0.15266152515316672,\n",
       " 0.1801015985633989,\n",
       " 0.1337361666522738,\n",
       " 0.10703171406481123,\n",
       " 0.17702583401052496,\n",
       " 0.1609165401148852,\n",
       " 0.19756051125445595,\n",
       " 0.13559439492497577,\n",
       " 0.07893356735221241,\n",
       " 0.15647797431952915,\n",
       " 0.1362327970763212,\n",
       " 0.13173401056086576,\n",
       " 0.060190284883388775,\n",
       " 0.13303298677231923,\n",
       " 0.12734598216285717,\n",
       " 0.1450655693813604,\n",
       " 0.03669885004964457,\n",
       " 0.25528384349404776,\n",
       " 0.1782284741164059,\n",
       " 0.1820429182836173,\n",
       " 0.14701657901105283,\n",
       " 0.07385856420294795,\n",
       " 0.14626176988934622,\n",
       " 0.1526872977848956,\n",
       " 0.08460679923491876,\n",
       " 0.1697461645404186,\n",
       " 0.18934621910711474,\n",
       " 0.0738676387406714,\n",
       " 0.24632546599846505,\n",
       " 0.12315780167890855,\n",
       " 0.022966378227149812,\n",
       " 0.16196715993078165,\n",
       " 0.12110795784659165,\n",
       " 0.09886780226801566,\n",
       " 0.07270505560905939,\n",
       " 0.12698549115030772,\n",
       " 0.22917396868437093,\n",
       " 0.07820913559090031,\n",
       " 0.1377388934776704,\n",
       " 0.11298874031453485,\n",
       " 0.028060370969143447,\n",
       " 0.13052173172884532,\n",
       " 0.18625435885485117,\n",
       " 0.12894899022583997,\n",
       " 0.061914855484879815,\n",
       " 0.15400011047288234,\n",
       " 0.1823144600571764,\n",
       " 0.0891425739193753,\n",
       " 0.1719843575318577,\n",
       " 0.11429114983545624,\n",
       " 0.12227288347180842,\n",
       " 0.19812454604366878,\n",
       " 0.0195235882893441,\n",
       " 0.201554579791084,\n",
       " 0.11693362722342074,\n",
       " 0.1310125344022861,\n",
       " 0.13509234469920495,\n",
       " 0.14126319235981336,\n",
       " 0.11708968840516626,\n",
       " 0.06097727298541099,\n",
       " 0.10014968486981274,\n",
       " 0.14642171162970954,\n",
       " 0.07246221447627024,\n",
       " 0.12452673504756726,\n",
       " 0.19127575679156522,\n",
       " 0.08789600561846456,\n",
       " 0.15265386731357494,\n",
       " 0.2008692366076687,\n",
       " 0.10737202449334379,\n",
       " 0.17586357537878572,\n",
       " 0.1561336625062127,\n",
       " 0.19398306728702963,\n",
       " 0.1820919214687702,\n",
       " 0.20178314390464966,\n",
       " 0.12979169858998002,\n",
       " 0.13357270803149698,\n",
       " 0.1438970906652248,\n",
       " 0.20299819984634987,\n",
       " 0.20856606188544113,\n",
       " 0.09070762736536314,\n",
       " 0.16007600970283337,\n",
       " 0.08947776410670762,\n",
       " 0.1489608194857588,\n",
       " 0.11027502218745824,\n",
       " 0.14377278840225632,\n",
       " 0.1613528169996624,\n",
       " 0.26267334211317384,\n",
       " 0.1711079597677108,\n",
       " 0.11641276865165937,\n",
       " 0.05604246531837438,\n",
       " 0.15111574109219836,\n",
       " 0.1577196536491888,\n",
       " 0.09454416489723624,\n",
       " 0.20953081716637956,\n",
       " 0.06465976689630847,\n",
       " 0.08796080912623835,\n",
       " 0.11850495420202312,\n",
       " 0.06740161617925702,\n",
       " 0.11752737746481359,\n",
       " 0.1796947254514105,\n",
       " 0.09418008335342426,\n",
       " 0.10166468627906543,\n",
       " 0.1383723403786654,\n",
       " 0.1501639997673894,\n",
       " 0.12935326114467655,\n",
       " 0.14485828559702332,\n",
       " 0.09321962441153558,\n",
       " 0.16621095173605668,\n",
       " 0.18568524178908466,\n",
       " 0.1550234361178456,\n",
       " 0.07539646132538955,\n",
       " 0.07902828055803347,\n",
       " 0.10396231962166474,\n",
       " 0.1569997979219867,\n",
       " 0.2037380445179798,\n",
       " 0.1167685412078654,\n",
       " 0.1586483395629976,\n",
       " 0.0778993467813793,\n",
       " 0.1832069102282014,\n",
       " 0.09791091474582322,\n",
       " 0.18253267358669492,\n",
       " 0.08004472908660865,\n",
       " 0.174237165486821,\n",
       " 0.11940814422758902,\n",
       " 0.11765265266033131,\n",
       " 0.13947613961376198,\n",
       " 0.26420103936917116,\n",
       " 0.1885271485450117,\n",
       " 0.14972836702239808,\n",
       " 0.08510121342397528,\n",
       " 0.22105979865646735,\n",
       " 0.1424223113050706,\n",
       " 0.09501967520265711,\n",
       " 0.1437715693614745,\n",
       " 0.13451520394943642,\n",
       " 0.11766167451036691,\n",
       " 0.10589088065103686,\n",
       " 0.12880787528941343,\n",
       " 0.20060374072792067,\n",
       " 0.1567271554012216,\n",
       " 0.21946606710378366,\n",
       " 0.08924206791031947,\n",
       " 0.16520203151518154,\n",
       " 0.12501255335642436,\n",
       " 0.09316460889729615,\n",
       " 0.11343885940328158,\n",
       " 0.1317664855524303,\n",
       " 0.1092405661754366,\n",
       " 0.14803731718098367,\n",
       " 0.22460219943378387,\n",
       " 0.06752727280148288,\n",
       " 0.11153677211548949,\n",
       " 0.07117964446359193,\n",
       " 0.1411663745540813,\n",
       " 0.08412625808158011,\n",
       " 0.1862700122615482,\n",
       " 0.10859662678260862,\n",
       " 0.0896595796017444,\n",
       " 0.08846169030664802,\n",
       " 0.060143130492356575,\n",
       " 0.17557999306032127,\n",
       " 0.22691596206076836,\n",
       " 0.13438067455246955,\n",
       " 0.18366068348158734,\n",
       " 0.0958983178999393,\n",
       " 0.22623298148949597,\n",
       " 0.1006051787541806,\n",
       " 0.1934718068665438,\n",
       " 0.10081876354394825,\n",
       " 0.13604931177638524,\n",
       " 0.0756840368783214,\n",
       " 0.19013982643185456,\n",
       " 0.1339488000864998,\n",
       " 0.20665685095666733,\n",
       " 0.11954056655452037,\n",
       " 0.12397700431083997,\n",
       " 0.10600511758257274,\n",
       " 0.2279696063191408,\n",
       " 0.1374370792317447,\n",
       " 0.08295334498506765,\n",
       " 0.14959280780823572,\n",
       " 0.046739429003719885,\n",
       " 0.10579643182315052,\n",
       " 0.1406793090059539,\n",
       " 0.22342050121005474,\n",
       " 0.2396139009948488,\n",
       " 0.20980229916157625,\n",
       " 0.11412961805702956,\n",
       " 0.11070158462462555,\n",
       " 0.12580026984771026,\n",
       " 0.1370127445412417,\n",
       " 0.06738119804107659,\n",
       " 0.19888373095517517,\n",
       " 0.20675352945231507,\n",
       " 0.15680905703369752,\n",
       " 0.06964559749151349,\n",
       " 0.06722547857092805,\n",
       " 0.10378178287674675,\n",
       " 0.16301220156815813,\n",
       " 0.06656958996497434,\n",
       " 0.15466631030343134,\n",
       " 0.15098908464239147,\n",
       " 0.20634605291165614,\n",
       " 0.21087965254961402,\n",
       " 0.1460591650516863,\n",
       " 0.15639146992306768,\n",
       " 0.20307643239203066,\n",
       " 0.19314666655686175,\n",
       " 0.26853502415844865,\n",
       " 0.12173516538565758,\n",
       " 0.1749242814286807,\n",
       " 0.08755162080329072,\n",
       " 0.08876323913218363,\n",
       " 0.1800822769371429,\n",
       " 0.2055477432539432,\n",
       " 0.14376976879811817,\n",
       " 0.09409933729034176,\n",
       " 0.0967136901169314,\n",
       " 0.07744268104069396,\n",
       " 0.12915030263615612,\n",
       " 0.12378253696396757,\n",
       " 0.1497358690431441,\n",
       " 0.15966377119201441,\n",
       " 0.17390460154646672,\n",
       " 0.1907862996876829,\n",
       " 0.1328065312689615,\n",
       " 0.24875719620710252,\n",
       " 0.08099971642646603,\n",
       " 0.1002765728715046,\n",
       " 0.17714224993930336,\n",
       " 0.1353707339101072,\n",
       " 0.09822187582575377,\n",
       " 0.21724339036006426,\n",
       " 0.1531455585474159,\n",
       " 0.08797202603929202,\n",
       " 0.2421732529100696,\n",
       " 0.11186282478927861,\n",
       " 0.11691121246687355,\n",
       " 0.0696639909289967,\n",
       " 0.014276466543570142,\n",
       " 0.22083635456608178,\n",
       " 0.07912315200810954,\n",
       " 0.13742710444190803,\n",
       " 0.2040668703851539,\n",
       " 0.1856907890229971,\n",
       " 0.1421593683959627,\n",
       " 0.12551935617762766,\n",
       " 0.01327009930381393,\n",
       " 0.0464734467833815,\n",
       " 0.1286601846333882,\n",
       " 0.0681547895672922,\n",
       " 0.061394414906943924,\n",
       " 0.08510763502397854,\n",
       " 0.044107229967187904,\n",
       " 0.19439172881339872,\n",
       " 0.20250927217688347,\n",
       " 0.11191960335182155,\n",
       " 0.11996225005374075,\n",
       " 0.07600200548345663,\n",
       " 0.20573817909080003,\n",
       " 0.1858456407268062,\n",
       " 0.03660141113270414,\n",
       " 0.06827121606717368,\n",
       " 0.228448498519638,\n",
       " 0.24189064950360065,\n",
       " 0.058643443247990465,\n",
       " 0.12781464649871874,\n",
       " 0.13335590245991144,\n",
       " 0.09059855386160792,\n",
       " 0.15093849271595136,\n",
       " 0.08989398773642301,\n",
       " 0.19421689118717228,\n",
       " 0.17564917508027778,\n",
       " 0.16768094262708888,\n",
       " 0.16096110465861696,\n",
       " 0.28192380440121895,\n",
       " 0.15074331879240624,\n",
       " 0.1555472056518029,\n",
       " 0.09239337995097172,\n",
       " 0.11466063284426725,\n",
       " 0.06969214863282551,\n",
       " 0.13633492545701176,\n",
       " 0.12724827713985767,\n",
       " 0.08751788738055183,\n",
       " 0.1428206597778022,\n",
       " 0.17925130571244136,\n",
       " 0.07643718121456428,\n",
       " 0.1576687128439988,\n",
       " 0.11861473866648614,\n",
       " 0.047697630243839294,\n",
       " 0.16882029434512422,\n",
       " 0.0794393992930289,\n",
       " 0.14190086937830193,\n",
       " 0.07035682903775159,\n",
       " 0.07248431725575846,\n",
       " 0.23632062533550396,\n",
       " 0.05971179930291197,\n",
       " 0.17182751229499152,\n",
       " 0.14884075486766804,\n",
       " 0.2070732432435228,\n",
       " 0.157836310552067,\n",
       " 0.15664799348049127,\n",
       " 0.046029666236500955,\n",
       " 0.11835863594576077,\n",
       " 0.14640390567316608,\n",
       " 0.09422877748499367,\n",
       " 0.15764861286845808,\n",
       " 0.14092048419435493,\n",
       " 0.12169067970941069,\n",
       " 0.14011268886991238,\n",
       " 0.19541906272618537,\n",
       " 0.1174436347544161,\n",
       " 0.06393588892562901,\n",
       " 0.15293333916169083,\n",
       " 0.12471289501300839,\n",
       " 0.1539087947057435,\n",
       " 0.06034857632000029,\n",
       " 0.09783509199081256,\n",
       " 0.07541306240975837,\n",
       " 0.10560002543749838,\n",
       " 0.13629074712727554,\n",
       " 0.09256220462162984,\n",
       " 0.1418274313179819,\n",
       " 0.11975462871056541,\n",
       " 0.11467188377196941,\n",
       " 0.05607232140081846,\n",
       " 0.1515495937390146,\n",
       " 0.12343280093178213,\n",
       " 0.169681838008268,\n",
       " 0.1487060399784877,\n",
       " 0.15297968473828233,\n",
       " 0.13767179639531946,\n",
       " 0.11315930798108353,\n",
       " 0.16454193422607646,\n",
       " 0.08571738008177653,\n",
       " 0.169443232905634,\n",
       " 0.18918510742680147,\n",
       " 0.08581424717156988,\n",
       " 0.14110245111914574,\n",
       " 0.12222866155799919,\n",
       " 0.10507847084321215,\n",
       " 0.038359148751875745,\n",
       " 0.1865694411648025,\n",
       " 0.11350916963761704,\n",
       " 0.06256755790704002,\n",
       " 0.15552758706274494,\n",
       " 0.05525949405507387,\n",
       " 0.16166665822832355,\n",
       " 0.13633362874043609,\n",
       " 0.1651315683340131,\n",
       " 0.22441529544511518,\n",
       " 0.14287560011628622,\n",
       " 0.09710053391355124,\n",
       " 0.0913126753411606,\n",
       " 0.04030415247388258,\n",
       " 0.19017235355553197,\n",
       " 0.15024548465213144,\n",
       " 0.25215799474737804,\n",
       " 0.1195464027985492,\n",
       " 0.12997723100098468,\n",
       " 0.24725559308139414,\n",
       " 0.10491469677985463,\n",
       " 0.09969911053903213,\n",
       " 0.1240007683455937,\n",
       " 0.13391639416813023,\n",
       " 0.05966882037080504,\n",
       " 0.08423780768286751,\n",
       " 0.2106962421965682,\n",
       " 0.2277614106874434,\n",
       " 0.13375884160092028,\n",
       " 0.15784665495777936,\n",
       " 0.09544567401855154,\n",
       " 0.17547188914145884,\n",
       " 0.15832847600423086,\n",
       " 0.19591321332686062,\n",
       " ...]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we may define a function to sort the value and print out the first k values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def man_dis(lst1,lst2,k,output):\n",
    "    length2 = []\n",
    "    for x in range(lst1.shape[0]):\n",
    "        lens = l1 = abs((lst1[x]-lst2)).sum()\n",
    "        length2.append(lens)\n",
    "    m_value = np.argsort(length2)\n",
    "    m_final = output[m_value]\n",
    "    return m_final[0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547700.0"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_dis(feature_train_norm,feature_testing_norm[0],5,output_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we still take the average of the prices of the k nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def man_mean(lst1,lst2,k,output):\n",
    "    valuem = man_dis(lst1,lst2,k,output).mean()\n",
    "    return valuem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662500.0"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_mean(feature_train_norm,feature_testing_norm[0],2,output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the defined function to calculate the SSR using the complete test set for k from 1 to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSR3 = []\n",
    "for k in range(1,15+1):\n",
    "    rr3 = []\n",
    "    for y in range(feature_testing_norm.shape[0]):\n",
    "        prediction = man_mean(feature_train_norm,feature_testing_norm[y],k,output_train)\n",
    "        actual = output_testing[y]\n",
    "        errorsquare = (prediction-actual) * (prediction-actual)\n",
    "        rr3.append(errorsquare)\n",
    "    rs1 = sum(rr3)\n",
    "    SSR3.append(rs1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = np.arange(1,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "datam = pd.DataFrame(SSR3,index=km,columns=['SSR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.956906e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SSR\n",
       "15  2.956906e+14"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datam[datam['SSR']==datam['SSR'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.687136e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.312549e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.608934e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.482609e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.280121e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.232229e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.245314e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.188569e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.091256e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.056941e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.011676e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.048854e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.981256e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.985591e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.956906e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SSR\n",
       "1   4.687136e+14\n",
       "2   4.312549e+14\n",
       "3   3.608934e+14\n",
       "4   3.482609e+14\n",
       "5   3.280121e+14\n",
       "6   3.232229e+14\n",
       "7   3.245314e+14\n",
       "8   3.188569e+14\n",
       "9   3.091256e+14\n",
       "10  3.056941e+14\n",
       "11  3.011676e+14\n",
       "12  3.048854e+14\n",
       "13  2.981256e+14\n",
       "14  2.985591e+14\n",
       "15  2.956906e+14"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I find that when k=15, the SSR is the smallest. And also, the SSRs may show us a \n",
    "# decreasing trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using SSR to compare models, the Manhattan Distance gives us the smallest SSR among all the methods. \n",
    "\n",
    "And according to my research, the cosine distance is more useful in test analysis. And Manhattan is more useful when dataset contains a lot of binary and discrete data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chebyshev Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Chebyshev Distance = Max({|x_1 - x_2|,|y_1 - y_2|,|z_1,z_2|})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will check to see one observation from testing and one from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = feature_testing_norm[0]\n",
    "c2 = feature_train_norm[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13281351307083628"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3 = c2-c1\n",
    "max(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, check the distance from one point testing and each points in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdis = []\n",
    "for cc in range(feature_train_norm.shape[0]):\n",
    "    c21 = feature_train_norm[cc]\n",
    "    c33 = max(abs(c21-c1))\n",
    "    cdis.append(c33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.13281351307083628,\n",
       " 0.01172101801893786,\n",
       " 0.027763990565187645,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.016409425226513005,\n",
       " 0.003124049717375413,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.1565302118334856,\n",
       " 0.01172101801893786,\n",
       " 0.01381395390224097,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.056920077030358396,\n",
       " 0.019922026960625436,\n",
       " 0.01172101801893786,\n",
       " 0.03226582631176453,\n",
       " 0.04009998175687518,\n",
       " 0.01172101801893786,\n",
       " 0.013102688576781608,\n",
       " 0.012351033511145554,\n",
       " 0.01666764976849467,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.04716408396116266,\n",
       " 0.012351033511145554,\n",
       " 0.056920077030358396,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.002451177470556093,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.018753628830300576,\n",
       " 0.016341183137040624,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.01172101801893786,\n",
       " 0.056920077030358396,\n",
       " 0.07304743218895994,\n",
       " 0.019922026960625436,\n",
       " 0.01172101801893786,\n",
       " 0.004527327658040273,\n",
       " 0.032690129581913197,\n",
       " 0.021323641917059873,\n",
       " 0.019922026960625436,\n",
       " 0.016409425226513005,\n",
       " 0.056920077030358396,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.019641461300011574,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.07304743218895994,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.006175516755572777,\n",
       " 0.18214424649714686,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.056920077030358396,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.013169071116320975,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.019321677095002227,\n",
       " 0.012351033511145554,\n",
       " 0.013462473583619673,\n",
       " 0.032578885709230884,\n",
       " 0.0426900577727688,\n",
       " 0.023342258657517833,\n",
       " 0.037507257660601145,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.014521058265956698,\n",
       " 0.019922026960625436,\n",
       " 0.018753628830300576,\n",
       " 0.02273346948182416,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.018753628830300576,\n",
       " 0.04778995029195311,\n",
       " 0.037507257660601145,\n",
       " 0.02361461170980184,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.056920077030358396,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.0070597808041377615,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.0426900577727688,\n",
       " 0.02013810510123536,\n",
       " 0.01172101801893786,\n",
       " 0.016291339099147702,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.013954088737610181,\n",
       " 0.014648441619308443,\n",
       " 0.03035737441619115,\n",
       " 0.056920077030358396,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.018753628830300576,\n",
       " 0.012351033511145554,\n",
       " 0.016557878628186612,\n",
       " 0.037507257660601145,\n",
       " 0.043304135313157646,\n",
       " 0.016409425226513005,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.01886563916183612,\n",
       " 0.03817748962310569,\n",
       " 0.03035737441619115,\n",
       " 0.001993019039891207,\n",
       " 0.012351033511145554,\n",
       " 0.06542881561928814,\n",
       " 0.09107212324857343,\n",
       " 0.016409425226513005,\n",
       " 0.017222325365018307,\n",
       " 0.017574782256209374,\n",
       " 0.014755127126680798,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.018753628830300576,\n",
       " 0.019922026960625436,\n",
       " 0.056920077030358396,\n",
       " 0.0217793972559045,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.015107584017871871,\n",
       " 0.01172101801893786,\n",
       " 0.01172101801893786,\n",
       " 0.004430363911841009,\n",
       " 0.0426900577727688,\n",
       " 0.013665714917544758,\n",
       " 0.037536658911849194,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.02034637508239372,\n",
       " 0.012351033511145554,\n",
       " 0.013565779304976346,\n",
       " 0.012116345038096898,\n",
       " 0.01172101801893786,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.013922047202047354,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.022845614856294046,\n",
       " 0.018311737574154347,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.4752879130056262,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.012351033511145554,\n",
       " 0.021916410324972133,\n",
       " 0.03056762492693481,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.0426900577727688,\n",
       " 0.03035737441619115,\n",
       " 0.07304743218895994,\n",
       " 0.019922026960625436,\n",
       " 0.016409425226513005,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.056920077030358396,\n",
       " 0.037507257660601145,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.01172101801893786,\n",
       " 0.015107584017871871,\n",
       " 0.029586472387358922,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.037507257660601145,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.006175516755572777,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.0426900577727688,\n",
       " 0.0426900577727688,\n",
       " 0.29495675052007525,\n",
       " 0.01172101801893786,\n",
       " 0.056920077030358396,\n",
       " 0.02156395343378106,\n",
       " 0.06329349730226444,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.0426900577727688,\n",
       " 0.016709660796013107,\n",
       " 0.007593843928389465,\n",
       " 0.019922026960625436,\n",
       " 0.06329349730226444,\n",
       " 0.025182734271142957,\n",
       " 0.09107212324857343,\n",
       " 0.003124049717375413,\n",
       " 0.016409425226513005,\n",
       " 0.0426900577727688,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.0426900577727688,\n",
       " 0.019922026960625436,\n",
       " 0.01172101801893786,\n",
       " 0.01739855381061384,\n",
       " 0.030423438016902105,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.018753628830300576,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.02580945689585534,\n",
       " 0.01172101801893786,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012530235143000846,\n",
       " 0.056920077030358396,\n",
       " 0.03609478981152209,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.0240792139754628,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.015464226215363888,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.013160099633540166,\n",
       " 0.01666764976849467,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.013393361865260748,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.05056393083856514,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.018753628830300576,\n",
       " 0.01172101801893786,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.018753628830300576,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.0426900577727688,\n",
       " 0.01369184298592879,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.003124049717375413,\n",
       " 0.018753628830300576,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.31974858219612967,\n",
       " 0.025024439274566133,\n",
       " 0.016837826938264407,\n",
       " 0.00879998205894197,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.0426900577727688,\n",
       " 0.013537548775293458,\n",
       " 0.020122084333453947,\n",
       " 0.018753628830300576,\n",
       " 0.012351033511145554,\n",
       " 0.09376814415150286,\n",
       " 0.039491192581181514,\n",
       " 0.012351033511145554,\n",
       " 0.015468207214591337,\n",
       " 0.012351033511145554,\n",
       " 0.07304743218895994,\n",
       " 0.023678694780927493,\n",
       " 0.015664324634526296,\n",
       " 0.018753628830300576,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.11099415020919888,\n",
       " 0.0584149835529716,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.018753628830300576,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.0426900577727688,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.015011459411183398,\n",
       " 0.012351033511145554,\n",
       " 0.0426900577727688,\n",
       " 0.09107212324857343,\n",
       " 0.01381395390224097,\n",
       " 0.01172101801893786,\n",
       " 0.016741702331575934,\n",
       " 0.023726757084271726,\n",
       " 0.07304743218895994,\n",
       " 0.037507257660601145,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.07304743218895994,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.0426900577727688,\n",
       " 0.0426900577727688,\n",
       " 0.017718969166242087,\n",
       " 0.003476506608566486,\n",
       " 0.4730622971416117,\n",
       " 0.019922026960625436,\n",
       " 0.07304743218895994,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.012983655833167133,\n",
       " 0.01172101801893786,\n",
       " 0.021403745755966933,\n",
       " 0.056920077030358396,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.02206059723500484,\n",
       " 0.03801728194529157,\n",
       " 0.012351033511145554,\n",
       " 0.07999169353259199,\n",
       " 0.012351033511145554,\n",
       " 0.02332623788973642,\n",
       " 0.03135264254822402,\n",
       " 0.019922026960625436,\n",
       " 0.016409425226513005,\n",
       " 0.008699276905306922,\n",
       " 0.018753628830300576,\n",
       " 0.019922026960625436,\n",
       " 0.013099443218012496,\n",
       " 0.030359354945776452,\n",
       " 0.017302429203925367,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.06329349730226444,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.01558820705131424,\n",
       " 0.019922026960625436,\n",
       " 0.008077841450038061,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.06329349730226444,\n",
       " 0.09107212324857343,\n",
       " 0.0426900577727688,\n",
       " 0.014787168662243624,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.037507257660601145,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.01946523285441604,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.07304743218895994,\n",
       " 0.037507257660601145,\n",
       " 0.0426900577727688,\n",
       " 0.016409425226513005,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.04457185918143644,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.009051733796497992,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.010365436754573808,\n",
       " 0.013922047202047354,\n",
       " 0.014434711771052551,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.008589272486648637,\n",
       " 0.009833582788923729,\n",
       " 0.018753628830300576,\n",
       " 0.056920077030358396,\n",
       " 0.012351033511145554,\n",
       " 0.01257063007878861,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.056920077030358396,\n",
       " 0.003556610447473546,\n",
       " 0.03035737441619115,\n",
       " 0.015267791695685994,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.016557878628186612,\n",
       " 0.021259558845934227,\n",
       " 0.0426900577727688,\n",
       " 0.056920077030358396,\n",
       " 0.019922026960625436,\n",
       " 0.022845614856294046,\n",
       " 0.016409425226513005,\n",
       " 0.018753628830300576,\n",
       " 0.016409425226513005,\n",
       " 0.013553569543074871,\n",
       " 0.037507257660601145,\n",
       " 0.158716354981792,\n",
       " 0.014546857145522438,\n",
       " 0.021676098808250947,\n",
       " 0.037507257660601145,\n",
       " 0.0426900577727688,\n",
       " 0.0426900577727688,\n",
       " 0.01727038766836254,\n",
       " 0.018753628830300576,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.016469349279291924,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.018776339839815307,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.07304743218895994,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.06329349730226444,\n",
       " 0.056920077030358396,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.030872019514781652,\n",
       " 0.016409425226513005,\n",
       " 0.023213916744050565,\n",
       " 0.0426900577727688,\n",
       " 0.015267791695685994,\n",
       " 0.07304743218895994,\n",
       " 0.014022853573948214,\n",
       " 0.13281351307083628,\n",
       " 0.056920077030358396,\n",
       " 0.03035737441619115,\n",
       " 0.07304743218895994,\n",
       " 0.020250250475705247,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.01172101801893786,\n",
       " 0.02062810284927009,\n",
       " 0.012351033511145554,\n",
       " 0.013665714917544758,\n",
       " 0.016132913155882264,\n",
       " 0.01727038766836254,\n",
       " 0.0426900577727688,\n",
       " 0.017126200758329827,\n",
       " 0.030615687230279052,\n",
       " 0.016409425226513005,\n",
       " 0.015892601639161077,\n",
       " 0.016409425226513005,\n",
       " 0.0426900577727688,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.013922047202047354,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012672427315097191,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.037507257660601145,\n",
       " 0.019922026960625436,\n",
       " 0.035614166778079714,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.01299395319022818,\n",
       " 0.018753628830300576,\n",
       " 0.01172101801893786,\n",
       " 0.0019823765672357975,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.022156721841693313,\n",
       " 0.463786066135872,\n",
       " 0.012795642991366884,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.010365436754573808,\n",
       " 0.00045719983042897747,\n",
       " 0.0426900577727688,\n",
       " 0.01172101801893786,\n",
       " 0.016409425226513005,\n",
       " 0.01835979987749858,\n",
       " 0.01172101801893786,\n",
       " 0.016409425226513005,\n",
       " 0.005007207216668964,\n",
       " 0.09107212324857343,\n",
       " 0.03035737441619115,\n",
       " 0.056920077030358396,\n",
       " 0.012351033511145554,\n",
       " 0.021788244182720833,\n",
       " 0.037507257660601145,\n",
       " 0.01525177092790458,\n",
       " 0.012384053495031768,\n",
       " 0.016409425226513005,\n",
       " 0.01172101801893786,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.0195453366933231,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.012629367293562496,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.015468207214591337,\n",
       " 0.07304743218895994,\n",
       " 0.037507257660601145,\n",
       " 0.032249805543983115,\n",
       " 0.037507257660601145,\n",
       " 0.02159599496934388,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.037507257660601145,\n",
       " 0.013791422866726268,\n",
       " 0.003141861117519181,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.01263933007582518,\n",
       " 0.012400074262813178,\n",
       " 0.01172101801893786,\n",
       " 0.014979417875620571,\n",
       " 0.021916410324972133,\n",
       " 0.012351033511145554,\n",
       " 0.013553569543074871,\n",
       " 0.037507257660601145,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.016773743867138757,\n",
       " 0.012351033511145554,\n",
       " 0.056920077030358396,\n",
       " 0.018753628830300576,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.008351625211942253,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.018753628830300576,\n",
       " 0.037507257660601145,\n",
       " 0.0426900577727688,\n",
       " 0.11099415020919888,\n",
       " 0.016409425226513005,\n",
       " 0.0426900577727688,\n",
       " 0.056920077030358396,\n",
       " 0.022156721841693313,\n",
       " 0.013898097647974122,\n",
       " 0.023358279425299246,\n",
       " 0.013489486471949221,\n",
       " 0.016409425226513005,\n",
       " 0.056920077030358396,\n",
       " 0.037507257660601145,\n",
       " 0.0013297237258572264,\n",
       " 0.0178631560762748,\n",
       " 0.03673562052277858,\n",
       " 0.012351033511145554,\n",
       " 0.0037969219641947327,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.0426900577727688,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.056920077030358396,\n",
       " 0.01172101801893786,\n",
       " 0.016409425226513005,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.02164250213124944,\n",
       " 0.03035737441619115,\n",
       " 0.037507257660601145,\n",
       " 0.012351033511145554,\n",
       " 0.0324901170607043,\n",
       " 0.030968144121470125,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.01381395390224097,\n",
       " 0.01879236060759672,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.0031479697256122964,\n",
       " 0.01332894817667687,\n",
       " 0.019922026960625436,\n",
       " 0.013585611078637694,\n",
       " 0.012351033511145554,\n",
       " 0.013403703508204455,\n",
       " 0.01172101801893786,\n",
       " 0.023758798619834553,\n",
       " 0.019138978005628308,\n",
       " 0.02399911013655574,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.009263275133359165,\n",
       " 0.03341793670271852,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.0035245689119107228,\n",
       " 0.019922026960625436,\n",
       " 0.018215612967465874,\n",
       " 0.012384053495031768,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.058155387046526924,\n",
       " 0.012810594333690136,\n",
       " 0.002611385148370216,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.07304743218895994,\n",
       " 0.016757723099357344,\n",
       " 0.012351033511145554,\n",
       " 0.015790942255247853,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.003124049717375413,\n",
       " 0.03035737441619115,\n",
       " 0.0028997589684356395,\n",
       " 0.06329349730226444,\n",
       " 0.019922026960625436,\n",
       " 0.016409425226513005,\n",
       " 0.02568129075360404,\n",
       " 0.012912738831818375,\n",
       " 0.04526324316205774,\n",
       " 0.0007850176212892065,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.018753628830300576,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.019922026960625436,\n",
       " 0.06329349730226444,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.037507257660601145,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.020794956580273267,\n",
       " 0.016409425226513005,\n",
       " 0.019176859034350614,\n",
       " 0.059292861559007204,\n",
       " 0.019922026960625436,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.03572390729959242,\n",
       " 0.016917930777171467,\n",
       " 0.09107212324857343,\n",
       " 0.037507257660601145,\n",
       " 0.07304743218895994,\n",
       " 0.012351033511145554,\n",
       " 0.018753628830300576,\n",
       " 0.01752671995286514,\n",
       " 0.0024191359349932696,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.014691044055555148,\n",
       " 0.03035737441619115,\n",
       " 0.013441288613108242,\n",
       " 0.09376814415150286,\n",
       " 0.01172101801893786,\n",
       " 0.09107212324857343,\n",
       " 0.03035737441619115,\n",
       " 0.09107212324857343,\n",
       " 0.01381395390224097,\n",
       " 0.019922026960625436,\n",
       " 0.022637344875135686,\n",
       " 0.019922026960625436,\n",
       " 0.07304743218895994,\n",
       " 0.03035737441619115,\n",
       " 0.06329349730226444,\n",
       " 0.01172101801893786,\n",
       " 0.018753628830300576,\n",
       " 0.01172101801893786,\n",
       " 0.016409425226513005,\n",
       " 0.09107212324857343,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.01261444014864175,\n",
       " 0.01172101801893786,\n",
       " 0.019922026960625436,\n",
       " 0.056920077030358396,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.01348441497596535,\n",
       " 0.01172101801893786,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.02774212408861619,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.02117945500702716,\n",
       " 0.03035737441619115,\n",
       " 0.014979417875620571,\n",
       " 0.016409425226513005,\n",
       " 0.012351033511145554,\n",
       " 0.02342804143691696,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.013040904974069675,\n",
       " 0.018753628830300576,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012709326067768999,\n",
       " 0.013489486471949221,\n",
       " 0.02319807174748512,\n",
       " 0.02213887694048949,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.018753628830300576,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.034012089999938475,\n",
       " 0.056920077030358396,\n",
       " 0.01172101801893786,\n",
       " 0.037507257660601145,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.0027975182510842633,\n",
       " 0.012656406547315778,\n",
       " 0.037507257660601145,\n",
       " 0.056920077030358396,\n",
       " 0.03641520516715033,\n",
       " 0.015235750160123171,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.037507257660601145,\n",
       " 0.0426900577727688,\n",
       " 0.052024933371168744,\n",
       " 0.013019740664135448,\n",
       " 0.016409425226513005,\n",
       " 0.016409425226513005,\n",
       " 0.07304743218895994,\n",
       " 0.01172101801893786,\n",
       " 0.0031011012738022606,\n",
       " 0.056920077030358396,\n",
       " 0.03035737441619115,\n",
       " 0.01172101801893786,\n",
       " 0.02238446106260471,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.07737837750117511,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.025200667720161666,\n",
       " 0.016409425226513005,\n",
       " 0.07304743218895994,\n",
       " 0.012448136566157418,\n",
       " 0.01659751542154322,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.004373669604325579,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.0426900577727688,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.018753628830300576,\n",
       " 0.0426900577727688,\n",
       " 0.007625885463952292,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.048943445572214805,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.022172742609474726,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.0426900577727688,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.09107212324857343,\n",
       " 0.020538624295770667,\n",
       " 0.03035737441619115,\n",
       " 0.0426900577727688,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.016409425226513005,\n",
       " 0.009276024545437765,\n",
       " 0.012351033511145554,\n",
       " 0.012351033511145554,\n",
       " 0.02298980176632676,\n",
       " 0.0426900577727688,\n",
       " 0.012351033511145554,\n",
       " 0.037507257660601145,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.037507257660601145,\n",
       " 0.056920077030358396,\n",
       " 0.002623647629914433,\n",
       " 0.03035737441619115,\n",
       " 0.03035737441619115,\n",
       " 0.012351033511145554,\n",
       " 0.03035737441619115,\n",
       " 0.019922026960625436,\n",
       " 0.013345299561916508,\n",
       " 0.009082697833218616,\n",
       " 0.01172101801893786,\n",
       " 0.012351033511145554,\n",
       " 0.0426900577727688,\n",
       " 0.0227815317851684,\n",
       " 0.056920077030358396,\n",
       " 0.01665975092244481,\n",
       " 0.010942184394704652,\n",
       " 0.018753628830300576,\n",
       " 0.016409425226513005,\n",
       " 0.03035737441619115,\n",
       " 0.18214424649714686,\n",
       " 0.02042647892130078,\n",
       " 0.03035737441619115,\n",
       " 0.016409425226513005,\n",
       " 0.013393361865260748,\n",
       " 0.0426900577727688,\n",
       " 0.0436245506687859,\n",
       " 0.045609879291151206,\n",
       " 0.013522302786719028,\n",
       " 0.024495753937779526,\n",
       " 0.012351033511145554,\n",
       " 0.019922026960625436,\n",
       " 0.019922026960625436,\n",
       " 0.012351033511145554,\n",
       " 0.01172101801893786,\n",
       " ...]"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we can write a function to sort the values and print out the first k values.\n",
    "\n",
    "As usual, we still take the average of the prices of the k nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch_k(c11,c22,k,output):\n",
    "    cids3 = []\n",
    "    for c4 in range(c11.shape[0]):\n",
    "        cdist = max(abs(c11[c4]-c22))\n",
    "        cids3.append(cdist)\n",
    "    c_val = np.argsort(cids3)\n",
    "    c_fin = output[c_val]\n",
    "    c_mea = (c_fin[0:k]).mean()\n",
    "    return c_mea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557500.0"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_k(feature_train_norm,feature_testing_norm[0],4,output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the defined function to calculate the SSR using the complete test set for k from 1 to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSR4 = []\n",
    "for k in range(1,15+1):\n",
    "    rr3 = []\n",
    "    for q in range(feature_testing_norm.shape[0]):\n",
    "        prediction2 = ch_k(feature_train_norm,feature_testing_norm[q],k,output_train)\n",
    "        actual2 = output_testing[q]\n",
    "        errorsquare2 = (prediction2-actual2) * (prediction2-actual2)\n",
    "        rr3.append(errorsquare2)\n",
    "    rs2 = sum(rr3)\n",
    "    SSR4.append(rs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[537520000575087.0,\n",
       " 390629149995106.25,\n",
       " 374433121027567.6,\n",
       " 352133535394308.25,\n",
       " 337285491454530.25,\n",
       " 323567366995709.0,\n",
       " 303919602152959.8,\n",
       " 314356084538104.25,\n",
       " 312391872299609.8,\n",
       " 311140328518733.56,\n",
       " 314896703473060.7,\n",
       " 313753019057105.2,\n",
       " 309900423998066.4,\n",
       " 310039446827625.5,\n",
       " 304288376975715.9]"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SSR4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "datac = pd.DataFrame(SSR4,index=km,columns=['SSR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.375200e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.906291e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.744331e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.521335e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.372855e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.235674e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.039196e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.143561e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.123919e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.111403e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.148967e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.137530e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.099004e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.100394e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.042884e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SSR\n",
       "1   5.375200e+14\n",
       "2   3.906291e+14\n",
       "3   3.744331e+14\n",
       "4   3.521335e+14\n",
       "5   3.372855e+14\n",
       "6   3.235674e+14\n",
       "7   3.039196e+14\n",
       "8   3.143561e+14\n",
       "9   3.123919e+14\n",
       "10  3.111403e+14\n",
       "11  3.148967e+14\n",
       "12  3.137530e+14\n",
       "13  3.099004e+14\n",
       "14  3.100394e+14\n",
       "15  3.042884e+14"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.039196e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SSR\n",
       "7  3.039196e+14"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datac[datac['SSR']==datac['SSR'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Chebyshev Distance, we get the lowest ssr when k=7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Mahalanobis Distance = D^2 = ((x-m)^T*C^{-1}*(x-m))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x is the vector of the observation.\n",
    "\n",
    "m is the vector of mean values of independent variables.\n",
    "\n",
    "c is the inverse covariance matrix of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanx = (feature_train_norm).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03106849, 0.02838374, 0.00943026, 0.02970585, 0.02898499,\n",
       "       0.03106634, 0.03040765, 0.02651831, 0.02646054, 0.02401087])"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we calculate the covariance of the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "covx = np.linalg.inv(np.cov(feature_train_norm.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the difference between one point from test and meanx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwf1 = feature_testing_norm[0]-meanx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.93889390e-18, -1.40131151e-02, -4.01488226e-03, -1.96372822e-03,\n",
       "       -1.66339571e-02, -2.37905131e-04, -8.30098582e-03, -2.97471870e-03,\n",
       "       -5.36270516e-03, -2.02161964e-02])"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lwf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the left two points dot multiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = np.dot(lwf1,covx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the right two points dot muliple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = np.dot(left,lwf1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can get the square_distance, we can get the distance using sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.284139699691828"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(x, y):\n",
    "    meanx = np.mean(x)\n",
    "    covariance = np.cov(np.transpose(y))\n",
    "    inv_covmat = np.linalg.inv(covariance)\n",
    "    x_minus_mn = x - meanx\n",
    "    D_square = np.dot(np.dot(x_minus_mn, inv_covmat), np.transpose(x_minus_mn))\n",
    "    return D_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.65098073e+30, 1.65098073e+30, 1.65098073e+30, ...,\n",
       "        1.65098073e+30, 1.65098073e+30, 1.65098073e+30],\n",
       "       [1.65098073e+30, 1.65098073e+30, 1.65098073e+30, ...,\n",
       "        1.65098073e+30, 1.65098073e+30, 1.65098073e+30],\n",
       "       [1.65098073e+30, 1.65098073e+30, 1.65098073e+30, ...,\n",
       "        1.65098073e+30, 1.65098073e+30, 1.65098073e+30],\n",
       "       ...,\n",
       "       [1.65098073e+30, 1.65098073e+30, 1.65098073e+30, ...,\n",
       "        1.65098073e+30, 1.65098073e+30, 1.65098073e+30],\n",
       "       [1.65098073e+30, 1.65098073e+30, 1.65098073e+30, ...,\n",
       "        1.65098073e+30, 1.65098073e+30, 1.65098073e+30],\n",
       "       [1.65098073e+30, 1.65098073e+30, 1.65098073e+30, ...,\n",
       "        1.65098073e+30, 1.65098073e+30, 1.65098073e+30]])"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mahalanobis(feature_train_norm,feature_testing_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function to print out the first k value's price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_dis(x1,x2,k,output):\n",
    "    pricek = []\n",
    "    mean1 = (x1).mean(axis=0)\n",
    "    differ = x2 - mean1\n",
    "    cov2 = np.linalg.inv(np.cov(x1.T))\n",
    "    left2 = np.dot(differ,cov2)\n",
    "    right2 = np.dot(left2,differ.T)\n",
    "    dist2 = sqrt(right2)\n",
    "    pricek.append(dist2)\n",
    "    pricek2 = np.argsort(pricek)\n",
    "    m_fin = output[pricek2]\n",
    "    m_mea = (m_fin[0:k]).mean()\n",
    "    return m_mea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1655000.0"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_dis(feature_train_norm,feature_testing_norm[0],10,output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, here is a problem. I can only get each point's distance in test set, because I use the whole training test to get the distance. And after that, I cannot sort to get each estimated's price.\n",
    "\n",
    "Therefore, I think that this method is not useful in KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
